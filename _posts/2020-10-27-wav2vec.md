---
published: true
title: Wav2Vec and Wav2Vec 2.0 tutorial, from idea to code
collection: ml
layout: single
author_profile: true
read_time: true
categories: [machinelearning]
excerpt : "Speech Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/lgen_head.png"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

You might have already heard of Fairseq, a sequence-to-sequence toolkit written in PyTorch by FacebookAI. One of the most common applications of Fairseq among speech processing enthousiasts is [Wav2Vec](https://arxiv.org/abs/1904.05862), a framework for unsupervised pre-trained for speech recognition. Don't worry, we'll cover this. A year later, [wav2vec 2.0](https://arxiv.org/abs/2006.11477) was released on [Github](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md), a framework for self-supervised learning of speech representations. End 2020, the authors released a last paper named [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430) that combines pre-training and self-training and achieves great results for ASR with only 10 minutes of labeled speech. This approach rivals the best published systems trained on 960 hours of labeled data from just a year ago.

Resources we will rely on:
- [Wav2Vec paper](https://arxiv.org/abs/1904.05862)
- [Wav2Vec2.0 paper](https://arxiv.org/abs/2006.11477)
- [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430)

# Wav2Vec

It is not new that speech recognition tasks require huge amounts of data, commonly hundreds of hours of labeled speech. Pre-training of neural networks has proven to be a great way to overcome limited amount of data on a new task.

## What is pre-training?

What we mean by **pre-training** is the fact of training a first neural network on a task where lots of data are available, saving the weights, and creating a second neural network by initializing the weights as the ones saved from the first one. This learns general representations on huge amounts of data, and can supposely improve the performance on the new task with limited data. This has been applied extensvely in Computer Vision, Natural Language Processing, and more recently, for certain speech tasks.

When pre-training, you can either do it:
- in a supervised fashion
- or in an unsupervised fashion

Supervised pre-training is clear. This is similar to transfer learning where you pre-train a model, knowing you $$ X $$ and $$ y $$. But for unsupervised pre-training, you learn a representation of speech. **wav2vec**, is a convolutional neural network (CNN) that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives.

## Pre-training approach

Given an input signal context (speech up to a certain time-stamp), the aim is to predict the next observations from this speech sample.

*Problem:* This usually requires being able to properly model $$ p(x) $$, the distribution of speech samples.

*Solution:* Lower the dimensionality of the speech sample through an "encoder network", and then use a *context network* to predict the next values.

More formally, given audio samples $$x_i \in X$$, we:
- learn a first *encoder network*, based on a CNN, that maps $$ X $$ to $$ Z $$: 

$$ f:X \to Z $$

- learn a second *context network*, based on a CNN too, that maps $$ Z $$ to a single contextualized tensor $$ C $$:

$$ g:Z \to C $$

A representation of these 2 networks is presented in the figure below:

![image](https://maelfabien.github.io/assets/images/wav0.png)


# Conclusion

I hope this Wav2Vec summary was useful. Feel free to leave a comment 

Additional references:
- [Wav2Vec explained, on YouTube](https://www.youtube.com/watch?v=XkUVOijzAt8)
- [Wav2Vec 2.0, on YouTube](https://www.youtube.com/watch?v=aUSXvoWfy3w)
