---
published: true
title: Self-training and pre-training with Wav2Vec and Wav2Vec 2.0
collection: ml
layout: single
author_profile: true
read_time: true
categories: [machinelearning]
excerpt : "Speech Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/lgen_head.png"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

You might have already heard of Fairseq, a sequence-to-sequence toolkit written in PyTorch by FacebookAI. One of the most common applications of Fairseq among speech processing enthousiasts is [Wav2Vec](https://arxiv.org/abs/1904.05862), a framework for unsupervised pre-trained for speech recognition. Don't worry, we'll cover this. A year later, [wav2vec 2.0](https://arxiv.org/abs/2006.11477) was released on [Github](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md), a framework for self-supervised learning of speech representations. End 2020, the authors released a last paper named [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430) that combines pre-training and self-training and achieves great results for ASR with only 10 minutes of labeled speech. This approach rivals the best published systems trained on 960 hours of labeled data from just a year ago.

Here are the 4 papers we will be reviewing, all of them coming from Facebook AI. All these papers are building blocks of what could be a great innovation in speech recognition but also a lot of other downstream tasks related to speech:
- [Wav2Vec paper](https://arxiv.org/abs/1904.05862)
- [vq - Wav2Vec](https://arxiv.org/abs/1910.05453)
- [Wav2Vec2.0 paper](https://arxiv.org/abs/2006.11477)
- [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430)

# 1. Wav2Vec

It is not new that speech recognition tasks require huge amounts of data, commonly hundreds of hours of labeled speech. Pre-training of neural networks has proven to be a great way to overcome limited amount of data on a new task.

## a. What is pre-training?

What we mean by **pre-training** is the fact of training a first neural network on a task where lots of data are available, saving the weights, and creating a second neural network by initializing the weights as the ones saved from the first one. This learns general representations on huge amounts of data, and can supposely improve the performance on the new task with limited data. This has been applied extensvely in Computer Vision, Natural Language Processing, and more recently, for certain speech tasks.

When pre-training, you can either do it:
- in a supervised fashion
- or in an unsupervised fashion

Supervised pre-training is clear. This is similar to transfer learning where you pre-train a model, knowing you $$ X $$ and $$ y $$. But for unsupervised pre-training, you learn a representation of speech. **wav2vec**, is a convolutional neural network (CNN) that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives.

## b. The model

Given an input signal context (speech up to a certain time-stamp), the aim is to predict the next observations from this speech sample.

*Problem:* This usually requires being able to properly model $$ p(x) $$, the distribution of speech samples.

*Solution:* Lower the dimensionality of the speech sample through an "encoder network", and then use a *context network* to predict the next values. wav2vec learns representations of audio data by solving a self-supervised context-prediction task.

More formally, given audio samples $$x_i \in X$$, we:
- learn a first *encoder network*, based on a CNN, that maps $$ X $$ to $$ Z $$: 

$$ f:X \to Z $$

- learn a second *context network*, based on a CNN too, that maps $$ Z $$ to a single contextualized tensor $$ C $$:

$$ g:Z \to C $$

A representation of these 2 networks is presented in the figure below:

![image](https://maelfabien.github.io/assets/images/wav0.png)

Here are the network details of Wav2Vec implementation:
- the encoder is a 5-layer CNN, with kernel sizes (10, 8, 4, 4, 4) and strides (5, 4, 2, 2, 2), and covers 30ms of audio. Layers have 512 channels, a group normalization layer and a ReLU nonlinearity. 
- the context network has nine layers with kernel size three and stride one, and the total receptive field of the context network is about 210 ms. Layers have 512 channels, a group normalization layer and a ReLU nonlinearity. 

## c. Loss function

The model learns to distinguish a true sample $$z_{ik} $$, $$ k $$ steps in the future, from a proposal distribution $$ p_n $$, called a constrastive loss, defined by:

$$ \mathcal{L}_{k}=-\sum_{i=1}^{T-k}\left(\log \sigma\left(\mathbf{z}_{i+k}^{\top} h_{k}\left(\mathbf{c}_{i}\right)\right)+\underset{\tilde{\mathbf{z}} \sim p_{n}}{\mathbb{E}}\left[\log \sigma\left(-\mathbf{\tilde { z }}^{\top} h_{k}\left(\mathbf{c}_{i}\right)\right)\right]\right) $$


We then optimize the loss over several time steps:

$$ \mathcal{L} = \sum_{k=1}^K \mathcal{L}_k $$


To get the expectation of the proposal distribution, we sample ten negative examples by choosing uniformly distractors from these negative audio sequences. In order words, we average ten uniformly sampled values from different audio samples. $$ \lambda $$ is the number of negative samples (10 lead to the best performance).

## d. Acoustic Models

Wav2Vec is used as an input to an acoustic model. The vector supposely carries more representation information than other types of features. It can be used as an input in a phoneme or grapheme-based wav2letter ASR model. The model then predicts the probabilities over 39-dimensional phoneme or 31-dimensional graphemes.

## e. Decoding

Regarding the language model decoding, the authors considered a 4-gram language model, a word-based convolutional language model, and a character based convolutional language model. Word sequences are decoded using beam-search.

## f. Results

Pre-training reduces WER by 36 % on nov92 when only about eight hours of transcribed data is available. It also improved the PER on the TIMIT database compared to a baseline system, and the more pre-training data, the better the results were (Librispeech + WSJ in their best system).

# 2. vq - Wav2Vec 

vq-Wav2Vec introduces self-supervised learning of discrete speech representations. Discretization, rather than enables the direct application of algorithms from the NLP community which require discrete inputs. 

Let's start with self-supervised learning. This method has become more and more popular in NLP and CV.

## a. What is self-supervised learning?

In self-supervised learning, we train a model using labels that are naturally part of the input data, rather than requiring separate external labels. For example, in an NLP model, we train a model to predict the next words. This information is part of the training data itself, and the model learns some information on the nature of language. Think of models like GPT or ULMFiT that do this in NLP. GPT-3 appears to be so good at this that you can use it for Question Answering on generic topics without fine-tuning, and get proper replies.

In Computer Vision, the implementation is slightly different. We still need to train a model in a self-supervised way using a "context task", with the idea to focus on a "downstream task". Several pretext tasks can be used, such as colorization, placing images in patches, placing frames in the right order... For videos for example, a common workflow is to train a model on one or multiple pretext tasks with unlabelled videos and then feed one intermediate feature layer of this model to fine-tune a simple model on downstream tasks of action classification, segmentation or object tracking.

Self-supervised training can allow you to use 1000x less training data for a given downstream task.

## b. Self-supervised learning in speech



# 3. Wav2Vec 2.0

Wav2Vec 2.0 leverages self-supervised training, like vq-Wav2Vec, but in a continuous framework from raw audio data.





# 4. Self-training and Pre-training are Complementary for Speech Recognition

This last work combines both self-supervised training and pre-training for speech recognition. It gained a lot of attention lately, especially on Twitter with this headline that just 10 minutes of labeled speech can reach the same WER than a whole system trained on 960 hours of data.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Great progress in speech recognition: wav2vec 2.0 pre-training + self-training with just 10 minutes of labeled data rivals the best published systems trained on 960 hours of labeled data from just a year ago.<br><br>Paper: <a href="https://t.co/niBzDiei1j">https://t.co/niBzDiei1j</a><br>Models: <a href="https://t.co/frCK1GJMZQ">https://t.co/frCK1GJMZQ</a> <a href="https://t.co/AjEWdna6J1">pic.twitter.com/AjEWdna6J1</a></p>&mdash; Michael Auli (@MichaelAuli) <a href="https://twitter.com/MichaelAuli/status/1320755019432427520?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



# Conclusion

I hope this Wav2Vec summary was useful. Feel free to leave a comment 

Additional references:
- [Wav2Vec explained, on YouTube](https://www.youtube.com/watch?v=XkUVOijzAt8)
- [Wav2Vec 2.0, on YouTube](https://www.youtube.com/watch?v=aUSXvoWfy3w)
